---
title: "Preventive Maintenance"
author: "Yeshil"
format: markdown_github
editor: visual
---

## Quarto

---
title: "Preventive Maintenance R code"
output:
  pdf_document: default
  html_document: default
date: "2024-02-07"
---

```{r}
chooseCRANmirror(graphics = FALSE)

install.packages("dplyr")
install.packages("tidyr")
install.packages("ggplot2")
install.packages("caret")
install.packages("randomForest")
install.packages("xgboost")
install.packages("keras")
install.packages("tfruns")
install.packages("tune")

```

```{r}
# Load required libraries
library(dplyr)
library(tidyr)
library(ggplot2)
library(caret)
library(randomForest)
library(xgboost)
library(keras)
library(tfruns)
library(lubridate)
```

```{r}
# Generate dummy sensor data
set.seed(123)
num_samples <- 10000

# Generate timestamps
timestamps <- seq(as.POSIXct("2024-01-01 00:00:00"), by = "hour", length.out = num_samples)

# Generate sensor readings
laser_power <- rnorm(num_samples, mean = 100, sd = 10)
gas_pressure <- rnorm(num_samples, mean = 50, sd = 5)
temperature <- rnorm(num_samples, mean = 25, sd = 2)
vibration <- rnorm(num_samples, mean = 0.1, sd = 0.05)

# Generate failure labels (0 for no failure, 1 for failure)
failure <- sample(0:1, num_samples, replace = TRUE, prob = c(0.9, 0.1))

# Combine data into a data frame
sensor_data <- data.frame(timestamp = timestamps,
                          laser_power = laser_power,
                          gas_pressure = gas_pressure,
                          temperature = temperature,
                          vibration = vibration,
                          failure = failure)

# Display the first few rows of the data
head(sensor_data)

```

```{r}
summary(sensor_data)
```

```{r}
# Check for missing values
missing_values <- sapply(sensor_data, function(x) sum(is.na(x)))
print(missing_values)

```

```{r}
# Impute missing values if necessary
# sensor_data <- na.omit(sensor_data)

# Feature engineering
# Assuming "timestamp" column represents time series data
sensor_data$timestamp <- as.POSIXct(sensor_data$timestamp)
sensor_data$hour <- hour(sensor_data$timestamp)
sensor_data$day <- wday(sensor_data$timestamp)
sensor_data$month <- month(sensor_data$timestamp)
```

```{r cars}
library(dplyr)
library(tidyr)
library(ggplot2)
library(caret)
library(randomForest)
library(xgboost)
library(tensorflow)
library(keras)
library(tfruns)
library(lubridate)

# Visualize data distribution and correlations
# Example plots are shown for illustration purposes


ggplot(sensor_data, aes(x = laser_power, fill = factor(failure))) +
  geom_density(alpha = 0.5) +
  labs(title = "Distribution of Laser Power by Failure Status")

ggplot(sensor_data, aes(x = laser_power, fill = factor(failure))) +
  geom_density(alpha = 0.5) +
  labs(title = "Distribution of Laser Power by Failure Status")

ggplot(sensor_data, aes(x = gas_pressure, fill = factor(failure))) +
  geom_density(alpha = 0.5) +
  labs(title = "Distribution of Gas Pressure by Failure Status")

ggplot(sensor_data, aes(x = temperature, fill = factor(failure))) +
  geom_density(alpha = 0.5) +
  labs(title = "Distribution of Temperature by Failure Status")

ggplot(sensor_data, aes(x = vibration, fill = factor(failure))) +
  geom_density(alpha = 0.5) +
  labs(title = "Distribution of Vibration by Failure Status")

# Model development
# Split data into training and testing sets
set.seed(123)
train_index <- sample(1:nrow(sensor_data), 0.8 * nrow(sensor_data))
train_data <- sensor_data[train_index, ]
test_data <- sensor_data[-train_index, ]

# Define features and target variable
features <- c("laser_power", "gas_pressure", "temperature", "vibration", "hour", "day", "month")
target <- "failure"

# Train machine learning models
# Random Forest
rf_model <- randomForest(as.factor(failure) ~ ., data = train_data[, c(features, target)], ntree = 100)

# XGBoost
xgb_model <- xgboost(data = as.matrix(train_data[, features]), label = train_data[, target], max_depth = 6, eta = 0.3, nthread = 2, nrounds = 100)
```

```{r}

# Neural Network with Keras
nn_model <- keras_model_sequential() %>%
  layer_dense(units = 128, activation = 'relu', input_shape = length(features)) %>%
  layer_dropout(rate = 0.2) %>%
  layer_dense(units = 64, activation = 'relu') %>%
  layer_dropout(rate = 0.2) %>%
  layer_dense(units = 1, activation = 'sigmoid')



```

```{r}
# Compile the model
nn_model %>% compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = c('accuracy'))

# Define early stopping callback
early_stopping <- callback_early_stopping(monitor = "val_loss", patience = 10)

# Train the model
history <- nn_model %>% fit(
  x = as.matrix(train_data[, features]),
  y = as.matrix(as.numeric(train_data[, target])),
  epochs = 100,
  batch_size = 128,
  validation_split = 0.2,
  callbacks = list(early_stopping)
)

```

```{r}
# Load required library
library(e1071)

# Train Support Vector Machine (SVM) model
svm_model <- svm(as.factor(failure) ~ ., data = train_data[, c(features, target)], kernel = "radial", cost = 1)

```

```{r}
# Model evaluation
# Random Forest
rf_pred <- predict(rf_model, test_data[, features])
rf_accuracy <- confusionMatrix(table(rf_pred, test_data[, target]))$overall["Accuracy"]

# XGBoost
xgb_pred <- predict(xgb_model, as.matrix(test_data[, features]))
xgb_accuracy <- sum(xgb_pred == test_data[, target]) / length(xgb_pred)

# Neural Network
nn_pred_prob <- nn_model %>% predict(as.matrix(test_data[, features]))
nn_pred <- ifelse(nn_pred_prob >= 0.5, 1, 0)  # Binary classification threshold
nn_accuracy <- sum(nn_pred == test_data[, target]) / length(nn_pred)



# SVM
svm_pred <- predict(svm_model, test_data[, features])
svm_accuracy <- sum(svm_pred == test_data[, target]) / length(svm_pred)

# Display model accuracies
print(paste("Support Vector Machine (SVM) Accuracy:", svm_accuracy))
print(paste("Random Forest Accuracy:", rf_accuracy))
print(paste("XGBoost Accuracy:", xgb_accuracy))
print(paste("Neural Network Accuracy:", nn_accuracy))

```
